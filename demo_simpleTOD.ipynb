{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo_simpleTOD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunesh/simpletod/blob/master/demo_simpleTOD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcJZo8JcgN6N",
        "outputId": "10603fb7-047f-4b06-a351-f2a505b73d83"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/40/866cbfac4601e0f74c7303d533a9c5d4a53858bd402e08e3e294dd271f25/transformers-4.2.1-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 32.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=d4e8f03bfaabfb9616e156c9118314919bba91a55571ec06f12682a4bf02e452\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsN_7G9xf4RF"
      },
      "source": [
        "import torch\r\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBYYYuOskDP_",
        "outputId": "f7ee6a4b-22f4-40f6-b442-69566a35ccaa"
      },
      "source": [
        "!pip install ipdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/44/8c/76b33b115f4f2c090e2809a0247fe777eb3832f9d606479bf0139b29ca2c/ipdb-0.13.4.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (51.1.1)\n",
            "Requirement already satisfied: ipython>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.3.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (0.2.0)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.4-cp36-none-any.whl size=10973 sha256=f2dda7ca831761ce6ff9025697723e3ee16a426a6dd7eefe9ce162e771c14f1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/51/e4/c91c61e3481a1a967beb18c4ea7a2b138a63cce94170b2e206\n",
            "Successfully built ipdb\n",
            "Installing collected packages: ipdb\n",
            "Successfully installed ipdb-0.13.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYdH174-kWhH",
        "outputId": "2f5fc70f-636e-470a-b841-285efa2d904a"
      },
      "source": [
        "!pip install colorama"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIfyaNt8rFMe",
        "outputId": "50278428-20e0-4c2b-f849-3545ebd23267"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 12.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 17.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 11.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (51.1.1)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdYA-BG3gxYd"
      },
      "source": [
        "import sys, os\r\n",
        "import json\r\n",
        "from collections import Counter\r\n",
        "import sqlite3\r\n",
        "\r\n",
        "import ipdb\r\n",
        "from colorama import Fore, Back, Style\r\n",
        "\r\n",
        "import random\r\n",
        "import pprint\r\n",
        "import time\r\n",
        "import logging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STFhcVy7pDdB"
      },
      "source": [
        "### In order to import customer module, we need to mount the drive first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwPzCriSpN8-",
        "outputId": "2727c497-888c-47d5-82dc-f9cf3982d19d"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rx-jypQpdvc"
      },
      "source": [
        "import sys\r\n",
        "sys.path.append('/content/gdrive/My Drive/code/simpleTOD/simpletod_code')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWfK55sbqfVC",
        "outputId": "366f4ba3-47d3-419d-9f95-e303b3db83a2"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGqHzcIHtmLK",
        "outputId": "6771c8df-722b-4784-c6f7-58105e16401b"
      },
      "source": [
        "#Check current directory\r\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8nz75D_kkP_"
      },
      "source": [
        "from utils.multiwoz.nlp import normalize, normalize_for_sql"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6Q9hXsJi-PH"
      },
      "source": [
        "logging.basicConfig(level=logging.INFO)\r\n",
        "logging.getLogger(\"transformers.file_utils\").setLevel(logging.ERROR)\r\n",
        "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\r\n",
        "logging.getLogger(\"transformers.modeling_gpt2\").setLevel(logging.ERROR)\r\n",
        "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.ERROR)\r\n",
        "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HP13ql2z-__",
        "outputId": "7629bc5b-7208-4839-86c7-9bcecb202a7d"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhACATYU0Msy",
        "outputId": "cd6b7bfd-ce28-4cf9-ddf1-4dacf09571ee"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD-_geOo1bXt",
        "outputId": "7ff5a3d9-7d24-4ad7-9255-2336888d5871"
      },
      "source": [
        "%cd MyDrive/code/simpleTOD/simpletod_code"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/code/simpleTOD/simpletod_code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9o2m7231kxJ",
        "outputId": "939753f0-d8f4-452a-8b5d-2fd2e3d0a93f"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/code/simpleTOD/simpletod_code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBZEt6gluzfa",
        "outputId": "d4b520fa-fa3b-4511-c787-9b7d8f4d2633"
      },
      "source": [
        "class MultiWozDB(object):\r\n",
        "    # loading databases\r\n",
        "    domains = ['restaurant', 'hotel', 'attraction', 'train', 'taxi', 'hospital']  # , 'police']\r\n",
        "    dbs = {}\r\n",
        "    #CUR_DIR = os.path.dirname(__file__)\r\n",
        "    dir='gdrive/My Drive/code/simpleTOD/simpletod_code/'\r\n",
        "\r\n",
        "    hotel_info = ['name', 'area', 'internet', 'parking', 'phone', 'postcode', 'pricerange', 'stars', 'takesbookings', 'type', 'address']\r\n",
        "    train_info = [\"arriveBy\", \"day\", \"departure\", \"destination\", \"duration\", \"leaveAt\", \"price\", \"trainID\"]\r\n",
        "    restaurant_info = [\"address\", \"area\", \"food\", \"id\", \"introduction\", \"name\", \"phone\", \"postcode\", \"pricerange\", \"signature\",\"type\"]\r\n",
        "    attraction_info = [\"address\", \"area\", \"entrance fee\", \"id\", \"name\", \"openhours\", \"phone\", \"postcode\", \"pricerange\", \"type\"]\r\n",
        "    taxi_info = []\r\n",
        "    database_keys = {'hotel': hotel_info, 'train': train_info, 'restaurant': restaurant_info, 'attraction': attraction_info}\r\n",
        "\r\n",
        "    for domain in domains:\r\n",
        "        db = os.path.join(dir+'utils/multiwoz/db/{}-dbase.db'.format(domain))\r\n",
        "        print(db)\r\n",
        "        conn = sqlite3.connect(db)\r\n",
        "        c = conn.cursor()\r\n",
        "        dbs[domain] = c\r\n",
        "\r\n",
        "    def queryResultVenues(self, domain, turn, real_belief=False):\r\n",
        "        # query the db\r\n",
        "\r\n",
        "        sql_query = \"select {} from {}\".format(','.join(self.database_keys[domain]), domain)\r\n",
        "        #     sql_query = \"select * from {}\".format(domain)\r\n",
        "\r\n",
        "        if real_belief == True:\r\n",
        "            items = turn.items()\r\n",
        "        else:\r\n",
        "            items = turn['metadata'][domain]['semi'].items()\r\n",
        "\r\n",
        "        flag = True\r\n",
        "        for key, val in items:\r\n",
        "            if val == \"\" or val == \"dontcare\" or val == 'not mentioned' or val == \"don't care\" or val == \"dont care\" or val == \"do n't care\":\r\n",
        "                pass\r\n",
        "            if 'book' in key:\r\n",
        "                pass\r\n",
        "            else:\r\n",
        "                if flag:\r\n",
        "                    sql_query += \" where \"\r\n",
        "                    val2 = val.replace(\"'\", \"''\")\r\n",
        "                    val2 = normalize_for_sql(val2)\r\n",
        "                    if key == 'leaveAt':\r\n",
        "                        sql_query += r\" \" + key + \" > \" + r\"'\" + val2 + r\"'\"\r\n",
        "                    elif key == 'arriveBy':\r\n",
        "                        sql_query += r\" \" + key + \" < \" + r\"'\" + val2 + r\"'\"\r\n",
        "                    else:\r\n",
        "                        sql_query += r\" \" + key + \"=\" + r\"'\" + val2 + r\"'\"\r\n",
        "                    flag = False\r\n",
        "                else:\r\n",
        "                    val2 = val.replace(\"'\", \"''\")\r\n",
        "                    val2 = normalize_for_sql(val2)\r\n",
        "                    if key == 'leaveAt':\r\n",
        "                        sql_query += r\" and \" + key + \" > \" + r\"'\" + val2 + r\"'\"\r\n",
        "                    elif key == 'arriveBy':\r\n",
        "                        sql_query += r\" and \" + key + \" < \" + r\"'\" + val2 + r\"'\"\r\n",
        "                    else:\r\n",
        "                        sql_query += r\" and \" + key + \"=\" + r\"'\" + val2 + r\"'\"\r\n",
        "\r\n",
        "        try:  # \"select * from attraction  where name = 'queens college'\"\r\n",
        "            results = self.dbs[domain].execute(sql_query).fetchall()\r\n",
        "            print(sql_query)\r\n",
        "            results_dic = []\r\n",
        "            for a in results:\r\n",
        "                a_dic = dict.fromkeys(self.database_keys[domain])\r\n",
        "                for k, v in zip(self.database_keys[domain], a):\r\n",
        "                    a_dic[k] = v\r\n",
        "                results_dic.append(a_dic)\r\n",
        "            print(results_dic)\r\n",
        "            return results_dic\r\n",
        "\r\n",
        "        except:\r\n",
        "            return []  # TODO test it\r\n",
        "\r\n",
        "    def queryResultVenues_new(self, domain, turn, real_belief=False):\r\n",
        "        # query the db\r\n",
        "        # sql_query = \"select * from {}\".format(domain)\r\n",
        "        sql_query = \"select {} from {}\".format(','.join(self.database_keys[domain]), domain)\r\n",
        "\r\n",
        "        if real_belief == True:\r\n",
        "            items = turn.items()\r\n",
        "        else:\r\n",
        "            items = turn['metadata'][domain]['semi'].items()\r\n",
        "\r\n",
        "        flag = True\r\n",
        "        for key, val in items:\r\n",
        "            if key == 'leaveat':\r\n",
        "                key = 'leaveAt'\r\n",
        "            if key == 'arriveby':\r\n",
        "                key = 'arriveBy'\r\n",
        "\r\n",
        "            if val == \"\" or val == \"dontcare\" or val == 'not mentioned' or val == \"don't care\" or val == \"dont care\" or val == \"do n't care\":\r\n",
        "                pass\r\n",
        "            if 'book' in key:\r\n",
        "                pass\r\n",
        "            else:\r\n",
        "                if flag:\r\n",
        "                    sql_query += \" where \"\r\n",
        "                    val2 = val.replace(\"'\", \"''\")\r\n",
        "                    val2 = normalize_for_sql(val2)\r\n",
        "\r\n",
        "                    # val2 = val2.replace('marys', r\"mary's\")\r\n",
        "                    # val2 = val2.replace('restaurant 17', 'restaurant one seven')\r\n",
        "                    # val2 = val2.replace('christ college', r\"christ's college\")\r\n",
        "                    # val2 = val2.replace('city centre north bed and breakfast', 'city centre north b and b')\r\n",
        "\r\n",
        "                    if key == 'name' and val2 in ['the cow pizza kitchen and bar',\r\n",
        "                                                  'cow pizza kitchen and bar',\r\n",
        "                                                  'wankworth house']:\r\n",
        "                        continue\r\n",
        "\r\n",
        "\r\n",
        "                    if key == 'leaveAt':\r\n",
        "                        sql_query += r\" \" + key + \" > \" + r\"'\" + val2 + r\"'\"\r\n",
        "                    elif key == 'arriveBy':\r\n",
        "                        sql_query += r\" \" + key + \" < \" + r\"'\" + val2 + r\"'\"\r\n",
        "                    else:\r\n",
        "                        sql_query += r\" \" + key + \"=\" + r\"'\" + val2 + r\"'\"\r\n",
        "                    flag = False\r\n",
        "                else:\r\n",
        "                    val2 = val.replace(\"'\", \"''\")\r\n",
        "                    val2 = normalize_for_sql(val2)\r\n",
        "\r\n",
        "                    # val2 = val2.replace('marys', r\"mary's\")\r\n",
        "                    # val2 = val2.replace('restaurant 17', 'restaurant one seven')\r\n",
        "                    # val2 = val2.replace('christ college', r\"christ's college\")\r\n",
        "                    # val2 = val2.replace('city centre north bed and breakfast', 'city centre north b and b')\r\n",
        "\r\n",
        "                    if key == 'name' and val2 in ['the cow pizza kitchen and bar',\r\n",
        "                                                  'cow pizza kitchen and bar',\r\n",
        "                                                  'wankworth house']:\r\n",
        "                        continue\r\n",
        "\r\n",
        "\r\n",
        "                    if key == 'leaveAt':\r\n",
        "                        sql_query += r\" and \" + key + \" > \" + r\"'\" + val2 + r\"'\"\r\n",
        "                    elif key == 'arriveBy':\r\n",
        "                        sql_query += r\" and \" + key + \" < \" + r\"'\" + val2 + r\"'\"\r\n",
        "                    else:\r\n",
        "                        sql_query += r\" and \" + key + \"=\" + r\"'\" + val2 + r\"'\"\r\n",
        "\r\n",
        "        if ('name', 'restaurant one seven') in list(turn.items()):\r\n",
        "            ipdb.set_trace()\r\n",
        "        try:  # \"select * from attraction  where name = 'queens college'\"\r\n",
        "            # return self.dbs[domain].execute(sql_query).fetchall()\r\n",
        "            results = self.dbs[domain].execute(sql_query).fetchall()\r\n",
        "            # print(sql_query)\r\n",
        "            results_dic = []\r\n",
        "            for a in results:\r\n",
        "                a_dic = dict.fromkeys(self.database_keys[domain])\r\n",
        "                for k, v in zip(self.database_keys[domain], a):\r\n",
        "                    a_dic[k] = v\r\n",
        "                results_dic.append(a_dic)\r\n",
        "            # print(results_dic)\r\n",
        "            return results_dic\r\n",
        "        except:\r\n",
        "            return []  # TODO test it"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive/My Drive/code/simpleTOD/simpletod_code/utils/multiwoz/db/restaurant-dbase.db\n",
            "gdrive/My Drive/code/simpleTOD/simpletod_code/utils/multiwoz/db/hotel-dbase.db\n",
            "gdrive/My Drive/code/simpleTOD/simpletod_code/utils/multiwoz/db/attraction-dbase.db\n",
            "gdrive/My Drive/code/simpleTOD/simpletod_code/utils/multiwoz/db/train-dbase.db\n",
            "gdrive/My Drive/code/simpleTOD/simpletod_code/utils/multiwoz/db/taxi-dbase.db\n",
            "gdrive/My Drive/code/simpleTOD/simpletod_code/utils/multiwoz/db/hospital-dbase.db\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoMS9O06w0UJ"
      },
      "source": [
        "def get_belief_new(sent):\r\n",
        "    if '<|belief|>' in sent:\r\n",
        "        tmp = sent.strip(' ').split('<|belief|>')[-1].split('<|action|>')[0]\r\n",
        "    else:\r\n",
        "        return []\r\n",
        "\r\n",
        "    tmp = tmp.strip(' .,')\r\n",
        "    # assert tmp.endswith('<endofbelief>')\r\n",
        "    tmp = tmp.replace('<|endofbelief|>', '')\r\n",
        "    tmp = tmp.replace('<|endoftext|>', '')\r\n",
        "    belief = tmp.split(',')\r\n",
        "    new_belief = []\r\n",
        "    for bs in belief:\r\n",
        "        bs = bs.strip(' .,')\r\n",
        "        if bs not in new_belief:\r\n",
        "            new_belief.append(bs)\r\n",
        "    return new_belief\r\n",
        "\r\n",
        "\r\n",
        "def get_belief_new_openaigpt(sent):\r\n",
        "    if '< | belief | >' in sent:\r\n",
        "        tmp = sent.strip(' ').split('< | belief | >')[-1].split('< | action | >')[0]\r\n",
        "    else:\r\n",
        "        return []\r\n",
        "\r\n",
        "    tmp = tmp.strip(' .,')\r\n",
        "    # assert tmp.endswith('<endofbelief>')\r\n",
        "    tmp = tmp.replace('< | endofbelief | >', '')\r\n",
        "    tmp = tmp.replace('<|endoftext|>', '')\r\n",
        "    belief = tmp.split(',')\r\n",
        "    new_belief = []\r\n",
        "    for bs in belief:\r\n",
        "        bs = bs.strip(' .,')\r\n",
        "        if bs not in new_belief:\r\n",
        "            new_belief.append(bs)\r\n",
        "    return new_belief\r\n",
        "\r\n",
        "\r\n",
        "def get_belief_new_dbsearch(sent):\r\n",
        "    if '<|belief|>' in sent:\r\n",
        "        tmp = sent.strip(' ').split('<|belief|>')[-1].split('<|endofbelief|>')[0]\r\n",
        "    else:\r\n",
        "        return []\r\n",
        "\r\n",
        "    tmp = tmp.strip(' .,')\r\n",
        "    # assert tmp.endswith('<endofbelief>')\r\n",
        "    tmp = tmp.replace('<|endofbelief|>', '')\r\n",
        "    tmp = tmp.replace('<|endoftext|>', '')\r\n",
        "    belief = tmp.split(',')\r\n",
        "    new_belief = []\r\n",
        "    for bs in belief:\r\n",
        "        bs = bs.strip(' .,')\r\n",
        "        if bs not in new_belief:\r\n",
        "            new_belief.append(bs)\r\n",
        "    return new_belief"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUzsCPOrxAj1"
      },
      "source": [
        "def get_action_new_openaigpt(sent):\r\n",
        "    if '< | belief | >' in sent:\r\n",
        "        tmp = sent.split('< | belief | >')[-1].split('< | response | >')[0].split('< | action | >')[-1].strip()\r\n",
        "    elif '< | action | >' in sent:\r\n",
        "        tmp = sent.split('< | response | >')[0].split('< | action | >')[-1].strip()\r\n",
        "    else:\r\n",
        "        return []\r\n",
        "    tmp = tmp.strip(' .,')\r\n",
        "    # if not tmp.endswith('<endofaction>'):\r\n",
        "    #     ipdb.set_trace()\r\n",
        "    tmp = tmp.replace('< | endofaction | >', '')\r\n",
        "    tmp = tmp.replace('< | endofbelief | >', '')\r\n",
        "    tmp = tmp.replace('<|endoftext|>', '')\r\n",
        "    action = tmp.split(',')\r\n",
        "    new_action = []\r\n",
        "    for act in action:\r\n",
        "        if act == '':\r\n",
        "            continue\r\n",
        "        act = act.strip(' .,')\r\n",
        "        if act not in new_action:\r\n",
        "            act = act.replace('i d', 'id')\r\n",
        "            new_action.append(act)\r\n",
        "    return new_action\r\n",
        "\r\n",
        "\r\n",
        "def get_action_new(sent):\r\n",
        "    if '<|action|>' not in sent:\r\n",
        "        return []\r\n",
        "    elif '<|belief|>' in sent:\r\n",
        "        tmp = sent.split('<|belief|>')[-1].split('<|response|>')[0].split('<|action|>')[-1].strip()\r\n",
        "    elif '<|action|>' in sent:\r\n",
        "        tmp = sent.split('<|response|>')[0].split('<|action|>')[-1].strip()\r\n",
        "    else:\r\n",
        "        return []\r\n",
        "    tmp = tmp.strip(' .,')\r\n",
        "    # if not tmp.endswith('<endofaction>'):\r\n",
        "    #     ipdb.set_trace()\r\n",
        "    tmp = tmp.replace('<|endofaction|>', '')\r\n",
        "    tmp = tmp.replace('<|endoftext|>', '')\r\n",
        "    action = tmp.split(',')\r\n",
        "    new_action = []\r\n",
        "    for act in action:\r\n",
        "        if act == '':\r\n",
        "            continue\r\n",
        "        act = act.strip(' .,')\r\n",
        "        if act not in new_action:\r\n",
        "            new_action.append(act)\r\n",
        "    return new_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDTVCPDjxFt5"
      },
      "source": [
        "def get_response_new(sent):\r\n",
        "    if '<|response|>' in sent:\r\n",
        "        tmp = sent.split('<|belief|>')[-1].split('<|action|>')[-1].split('<|response|>')[-1]\r\n",
        "    else:\r\n",
        "        return ''\r\n",
        "    # if '<belief>' in sent:\r\n",
        "    #     tmp = sent.split('<belief>')[-1].split('<action>')[-1].split('<response>')[-1]\r\n",
        "    # elif '<action>' in sent:\r\n",
        "    #     tmp = sent.split('<action>')[-1].split('<response>')[-1]\r\n",
        "    # elif '<response>' in sent:\r\n",
        "    #     tmp = sent.split('<response>')[-1]\r\n",
        "    # else:\r\n",
        "    #     tmp = sent\r\n",
        "    tmp = tmp.strip(' .,')\r\n",
        "    # assert tmp.endswith('<endofresponse>')\r\n",
        "    tmp = tmp.replace('<|endofresponse|>', '')\r\n",
        "    tmp = tmp.replace('<|endoftext|>', '')\r\n",
        "    tokens = tokenizer.encode(tmp)\r\n",
        "    new_tokens = []\r\n",
        "    for tok in tokens:\r\n",
        "        # if tok in break_tokens:\r\n",
        "        if tok in tokenizer.encode(tokenizer._eos_token):\r\n",
        "            continue\r\n",
        "        new_tokens.append(tok)\r\n",
        "    # ipdb.set_trace()\r\n",
        "    response = tokenizer.decode(new_tokens).strip(' ,.')\r\n",
        "    return response\r\n",
        "\r\n",
        "\r\n",
        "def convert_belief(belief):\r\n",
        "    dic = {}\r\n",
        "    for bs in belief:\r\n",
        "        if bs in [' ', '']:\r\n",
        "            continue\r\n",
        "        domain = bs.split(' ')[0]\r\n",
        "        slot = bs.split(' ')[1]\r\n",
        "        if slot == 'book':\r\n",
        "            slot = ' '.join(bs.split(' ')[1:3])\r\n",
        "            value = ' '.join(bs.split(' ')[3:])\r\n",
        "        else:\r\n",
        "            value = ' '.join(bs.split(' ')[2:])\r\n",
        "        if domain not in dic:\r\n",
        "            dic[domain] = {}\r\n",
        "        try:\r\n",
        "            dic[domain][slot] = value\r\n",
        "        except:\r\n",
        "            print(domain)\r\n",
        "            print(slot)\r\n",
        "    return dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSVE2MAFxWq1"
      },
      "source": [
        "def get_db_text(belief_domain, dom, only_match=False):\r\n",
        "    db_text_tmp = []\r\n",
        "    # for dom in belief_domain:\r\n",
        "    if dom not in ['restaurant', 'hotel', 'attraction', 'train']:\r\n",
        "        db_text_tmp = ''\r\n",
        "    domain_match = len(multiwoz_db.queryResultVenues_new(dom, belief_domain[dom], real_belief=True))\r\n",
        "\r\n",
        "    if dom != 'train':\r\n",
        "        if domain_match >= 5:\r\n",
        "            domain_match_text = '>=5'\r\n",
        "        else:\r\n",
        "            domain_match_text = '={}'.format(domain_match)\r\n",
        "\r\n",
        "    elif dom == 'train':\r\n",
        "        if domain_match == 0:\r\n",
        "            domain_match_text = '=0'\r\n",
        "        elif domain_match == 2:\r\n",
        "            domain_match_text = '<3'\r\n",
        "        elif domain_match == 5:\r\n",
        "            domain_match_text = '<6'\r\n",
        "        elif domain_match == 10:\r\n",
        "            domain_match_text = '<11'\r\n",
        "        elif domain_match == 40:\r\n",
        "            domain_match_text = '<41'\r\n",
        "        else:\r\n",
        "            domain_match_text = '>40'\r\n",
        "\r\n",
        "    if domain_match == 0:\r\n",
        "        domain_book_text = 'not available'\r\n",
        "    else:\r\n",
        "        domain_book_text = 'available'\r\n",
        "\r\n",
        "\r\n",
        "    # if USE_DB_BOOK_DYNAMIC:\r\n",
        "    if only_match:\r\n",
        "        db_text_tmp.append('{} match{}'.format(dom, domain_match_text))\r\n",
        "    else:\r\n",
        "        db_text_tmp.append('{} match{} booking={}'.format(dom, domain_match_text, domain_book_text))\r\n",
        "\r\n",
        "    return db_text_tmp\r\n",
        "\r\n",
        "\r\n",
        "def lexicalize_train(delex_response, db_results, turn_beliefs, turn_domain):\r\n",
        "    if len(db_results) > 0:\r\n",
        "        sample = random.sample(db_results, k=1)[0]\r\n",
        "        value_count = len(db_results)\r\n",
        "    else:\r\n",
        "        # domain = list(beliefs.keys())[0]\r\n",
        "        sample = turn_beliefs[turn_domain]\r\n",
        "        value_count = 0\r\n",
        "\r\n",
        "    # print(sample)\r\n",
        "    lex_response = delex_response\r\n",
        "\r\n",
        "    if 'from [value_place] to [value_place]' in delex_response:\r\n",
        "        departure = sample['departure']\r\n",
        "        destination = sample['destination']\r\n",
        "        lex_response = lex_response.replace('from [value_place] to [value_place]', 'from {} to {}'.format(departure, destination))\r\n",
        "    if 'from [value_place] on [value_day]' in delex_response:\r\n",
        "        departure = sample['departure']\r\n",
        "        day = sample['day']\r\n",
        "        lex_response = lex_response.replace('from [value_place] on [value_day]', 'from {} on {}'.format(departure, day))\r\n",
        "\r\n",
        "    if 'from [value_place]' in delex_response:\r\n",
        "        departure = sample['departure']\r\n",
        "        # destination = sample['destination']\r\n",
        "        lex_response = lex_response.replace('from [value_place]', 'from {}'.format(departure))\r\n",
        "\r\n",
        "    if 'leaving [value_place] at [value_day]' in delex_response:\r\n",
        "        departure = sample['departure']\r\n",
        "        day = sample['day']\r\n",
        "        lex_response = lex_response.replace('leaving [value_place] at [value_day]', 'leaving {} at {}'.format(departure, day))\r\n",
        "\r\n",
        "    if 'leaving [value_place] at [value_time]' in delex_response:\r\n",
        "        leaveat = sample['leaveAt']\r\n",
        "        departure = sample['departure']\r\n",
        "        lex_response = lex_response.replace('leaving [value_place] at [value_time]', 'leaving {} at {}'.format(departure, leaveat))\r\n",
        "    if 'leaves [value_place] at [value_time]' in delex_response:\r\n",
        "        leaveat = sample['leaveAt']\r\n",
        "        departure = sample['departure']\r\n",
        "        lex_response = lex_response.replace('leaves [value_place] at [value_time]', 'leaves {} at {}'.format(departure, leaveat))\r\n",
        "    if 'leaves at [value_time]' in delex_response:\r\n",
        "        if 'leaveAt' in sample:\r\n",
        "            leaveat = sample['leaveAt']\r\n",
        "            lex_response = lex_response.replace('leaves at [value_time]', 'leaves at {}'.format(leaveat))\r\n",
        "    if 'other at [value_time]' in delex_response:\r\n",
        "        leaveat = sample['leaveAt']\r\n",
        "        lex_response = lex_response.replace('other at [value_time]', 'other at {}'.format(leaveat))\r\n",
        "\r\n",
        "    if 'arrives in [value_place] at [value_time]' in delex_response:\r\n",
        "        arriveby = sample['arriveBy']\r\n",
        "        destination = sample['destination']\r\n",
        "        lex_response = lex_response.replace('arrives in [value_place] at [value_time]', 'arrives in {} at {}'.format(destination, arriveby))\r\n",
        "    if 'arrives at [value_time]' in delex_response:\r\n",
        "        arriveby = sample['arriveBy']\r\n",
        "        lex_response = lex_response.replace('arrives at [value_time]', 'arrives at {}'.format(arriveby))\r\n",
        "\r\n",
        "    if '[value_count] of these' in delex_response:\r\n",
        "        value_count = 'one'\r\n",
        "        lex_response = lex_response.replace('[value_count] of these', value_count)\r\n",
        "    if '[value_count] minutes' in delex_response:\r\n",
        "        lex_response = lex_response.replace('[value_count] minutes', sample['duration'])\r\n",
        "    if '[value_count]' in delex_response:\r\n",
        "        value_count = str(value_count)\r\n",
        "        lex_response = lex_response.replace('[value_count]', value_count)\r\n",
        "    if 'leaving [value_place]' in delex_response:\r\n",
        "        departure = sample['departure']\r\n",
        "        lex_response = lex_response.replace('leaving [value_place]', 'leaving {}'.format(departure))\r\n",
        "    if 'leaves [value_place]' in delex_response:\r\n",
        "        departure = sample['departure']\r\n",
        "        lex_response = lex_response.replace('leaves [value_place]', 'leaves {}'.format(departure))\r\n",
        "    if 'arrives in [value_place]' in delex_response:\r\n",
        "        destination = sample['destination']\r\n",
        "        lex_response = lex_response.replace('arrives in [value_place]', 'arrives in {}'.format(destination))\r\n",
        "    if '[train_id]' in delex_response:\r\n",
        "        train_id = sample['trainID']\r\n",
        "        lex_response = lex_response.replace('[train_id]', train_id)\r\n",
        "    if '[value_day]' in delex_response:\r\n",
        "        train_day = sample['day']\r\n",
        "        lex_response = lex_response.replace('[value_day]', train_day)\r\n",
        "    if '[value_price]' in delex_response:\r\n",
        "        train_price = sample['price']\r\n",
        "        lex_response = lex_response.replace('[value_price]', train_price)\r\n",
        "    if '[train_reference]' in delex_response:\r\n",
        "        random_number = random.randint(10000,99999)\r\n",
        "        lex_response = lex_response.replace('[train_reference]', str(random_number))\r\n",
        "\r\n",
        "    return lex_response\r\n",
        "\r\n",
        "\r\n",
        "def lexicalize_hotel(delex_response, db_results, turn_beliefs, turn_domain):\r\n",
        "    if len(db_results) > 0:\r\n",
        "        sample = random.sample(db_results, k=1)[0]\r\n",
        "        value_count = len(db_results)\r\n",
        "    else:\r\n",
        "        # ipdb.set_trace()\r\n",
        "        # domain = list(beliefs.keys())[0]\r\n",
        "        sample = turn_beliefs[turn_domain]\r\n",
        "        value_count = 0\r\n",
        "\r\n",
        "    # print(sample)\r\n",
        "    lex_response = delex_response\r\n",
        "    try:\r\n",
        "        if '[hotel_name]' in delex_response:\r\n",
        "            lex_response = lex_response.replace('[hotel_name]', sample['name'])\r\n",
        "        if '[hotel_address]' in delex_response:\r\n",
        "            lex_response = lex_response.replace('[hotel_address]', sample['address'])\r\n",
        "        if '[value_area]' in delex_response:\r\n",
        "            lex_response = lex_response.replace('[value_area]', sample['area'])\r\n",
        "        if 'starting [value_day]' in delex_response:\r\n",
        "            lex_response = lex_response.replace('starting [value_day]', 'starting {}'.format(beliefs['book day']))\r\n",
        "        if '[value_pricerange]' in delex_response:\r\n",
        "            lex_response = lex_response.replace('[value_pricerange]', sample['pricerange'])\r\n",
        "        if '[value_count] star' in delex_response:\r\n",
        "            lex_response = lex_response.replace('[value_count] star', '{} star'.format(sample['stars']))\r\n",
        "        if '[value_count]' in delex_response:\r\n",
        "            lex_response = lex_response.replace('[value_count]', str(value_count))\r\n",
        "        if '[hotel_reference]' in delex_response:\r\n",
        "            random_number = random.randint(10000, 99999)\r\n",
        "            lex_response = lex_response.replace('[hotel_reference]', str(random_number))\r\n",
        "        if 'starting [value_day]' in delex_response:\r\n",
        "            lex_response = lex_response.replace('starting [value_day]', 'starting {}'.format(beliefs['book day']))\r\n",
        "        if '[value_count] people' in delex_response:\r\n",
        "            lex_response = lex_response.replace('[value_count] people', '{} people'.format(beliefs['book people']))\r\n",
        "        if '[value_count] nights' in delex_response:\r\n",
        "            lex_response = lex_response.replace('[value_count] nights', '{} nights'.format(beliefs['book stay']))\r\n",
        "    except:\r\n",
        "        ipdb.set_trace()\r\n",
        "\r\n",
        "    return lex_response\r\n",
        "\r\n",
        "\r\n",
        "def get_turn_domain_old(b, a):\r\n",
        "    tmp = {}\r\n",
        "    turn_domain = None\r\n",
        "    if a == b:\r\n",
        "        turn_domain = list(a.keys())[0]\r\n",
        "    # elif len(b.keys()) > len(a.keys()):\r\n",
        "    #     turn_domain = list(set(b) - set(a))[0]\r\n",
        "    else:\r\n",
        "        for domain in b.keys():\r\n",
        "            if domain not in a:\r\n",
        "                turn_domain = domain\r\n",
        "                tmp = b\r\n",
        "                break\r\n",
        "            tmp = {k: b[domain][k] for k in set(b[domain]) - set(a[domain])}\r\n",
        "            if tmp != {}:\r\n",
        "                turn_domain = domain\r\n",
        "                break\r\n",
        "    if not turn_domain:\r\n",
        "        ipdb.set_trace()\r\n",
        "    print('domain change')\r\n",
        "    print('chane', tmp)\r\n",
        "    print(b)\r\n",
        "    print(a)\r\n",
        "    # domain = list(tmp.keys())\r\n",
        "    # if len(domain) > 1:\r\n",
        "    #     raise TypeError()\r\n",
        "    # elif len(domain) == 0:\r\n",
        "    #     domain = list(a.keys())[0]\r\n",
        "    # else:\r\n",
        "    #     domain = domain[0]\r\n",
        "    return turn_domain\r\n",
        "\r\n",
        "\r\n",
        "def get_turn_domain(beliefs, q):\r\n",
        "    for k in beliefs.keys():\r\n",
        "        if k not in q:\r\n",
        "            q.append(k)\r\n",
        "            turn_domain = k\r\n",
        "            return turn_domain\r\n",
        "    return q[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIwEbXCayCKk"
      },
      "source": [
        "pp = pprint.PrettyPrinter(indent=4)\r\n",
        "prev_beliefs = {}\r\n",
        "domain_queue = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyNjATd1fzb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "874dbb64-0dfd-4e26-c50b-d6d829b09bc4"
      },
      "source": [
        "def main(model_checkpoint,decoding,topp):\r\n",
        "\r\n",
        "    print('\\33]0;SimpleTOD\\a', end='')\r\n",
        "    sys.stdout.flush()\r\n",
        "    if decoding == 'nucleus':\r\n",
        "        TOP_P = float(topp)\r\n",
        "\r\n",
        "    delay = 0.5\r\n",
        "    multiwoz_db = MultiWozDB()\r\n",
        "    print('\\nLoading Model', end=\"\")\r\n",
        "\r\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint)\r\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_checkpoint)\r\n",
        "\r\n",
        "    # model.load_state_dict(torch.load(model_checkpoint))\r\n",
        "    model.eval()\r\n",
        "    model.to('cuda')\r\n",
        "\r\n",
        "    break_tokens = tokenizer.encode(tokenizer._eos_token) + tokenizer.encode('?') + tokenizer.encode('!')\r\n",
        "    MAX_LEN = model.config.n_ctx\r\n",
        "\r\n",
        "    if 'openai-gpt' in model_checkpoint:\r\n",
        "        tokenizer.add_special_tokens({'bos_token': '<|endoftext|>'})\r\n",
        "        tokenizer.add_special_tokens({'eos_token': '<|endoftext|>'})\r\n",
        "\r\n",
        "    sample = 1\r\n",
        "    print()\r\n",
        "    print(Fore.MAGENTA + '\\nSimpleTOD is ready to chat. What would you like to ask?' + Style.RESET_ALL)\r\n",
        "    # history = []\r\n",
        "    context = ''\r\n",
        "    input_text = ''\r\n",
        "    turn = 0\r\n",
        "    # dbmatch = 0\r\n",
        "\r\n",
        "    while True:\r\n",
        "        print(Fore.GREEN)\r\n",
        "        raw_text = input('You: ')\r\n",
        "        print(Style.RESET_ALL)\r\n",
        "        input_text = raw_text.replace('you> ', '')\r\n",
        "        if input_text in ['q', 'quit']:\r\n",
        "            break\r\n",
        "        user = '<|user|> {}'.format(input_text)\r\n",
        "        context = context + ' ' + user\r\n",
        "        text = '<|endoftext|> <|context|> {} <|endofcontext|>'.format(context)\r\n",
        "\r\n",
        "        # print(context)\r\n",
        "\r\n",
        "        text = text.strip()\r\n",
        "        indexed_tokens = tokenizer.encode(text)\r\n",
        "\r\n",
        "        if len(indexed_tokens) > MAX_LEN:\r\n",
        "            indexed_tokens = indexed_tokens[-1*MAX_LEN:]\r\n",
        "        # Convert indexed tokens in a PyTorch tensor\r\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\r\n",
        "\r\n",
        "        # If you have a GPU, put everything on cuda\r\n",
        "        tokens_tensor = tokens_tensor.to('cuda')\r\n",
        "        predicted_index = indexed_tokens[-1]\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            while predicted_index not in break_tokens:\r\n",
        "                outputs = model(tokens_tensor)\r\n",
        "                predictions = outputs[0]\r\n",
        "                predicted_index = torch.argmax(predictions[0, -1, :]).item()\r\n",
        "                indexed_tokens += [predicted_index]\r\n",
        "                tokens_tensor = torch.tensor([indexed_tokens]).to('cuda')\r\n",
        "                if len(indexed_tokens) > MAX_LEN:\r\n",
        "                    break\r\n",
        "                if tokenizer.decode(indexed_tokens).endswith('<|endofbelief|>'):\r\n",
        "                    break\r\n",
        "\r\n",
        "        tmp_pred = tokenizer.decode(indexed_tokens)\r\n",
        "        belief_text = get_belief_new_dbsearch(tmp_pred)\r\n",
        "        # print(tmp_pred)\r\n",
        "        beliefs = convert_belief(belief_text)\r\n",
        "        # domain = list(beliefs.keys())[0]\r\n",
        "        domain = get_turn_domain(beliefs, domain_queue)\r\n",
        "\r\n",
        "        if 'db' in model_checkpoint:\r\n",
        "            if 'dbnmatch' in model_checkpoint:\r\n",
        "                only_match = True\r\n",
        "                db_text_tmp = get_db_text(beliefs, dom=domain, only_match=only_match)\r\n",
        "            else:\r\n",
        "                db_text_tmp = get_db_text(beliefs, dom=domain)\r\n",
        "            db_text = ' <|dbsearch|> {} <|endofdbsearch|>'.format(' , '.join(db_text_tmp))\r\n",
        "            text = tmp_pred + db_text\r\n",
        "        # print(text)\r\n",
        "\r\n",
        "        # continue generation after creating db\r\n",
        "        indexed_tokens = tokenizer.encode(text)\r\n",
        "        if len(indexed_tokens) > MAX_LEN:\r\n",
        "            indexed_tokens = indexed_tokens[-1 * MAX_LEN:]\r\n",
        "\r\n",
        "        # Convert indexed tokens in a PyTorch tensor\r\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\r\n",
        "\r\n",
        "        # If you have a GPU, put everything on cuda\r\n",
        "        tokens_tensor = tokens_tensor.to('cuda')\r\n",
        "        predicted_index = indexed_tokens[-1]\r\n",
        "\r\n",
        "        truncate_action = False\r\n",
        "        # Predict all tokens\r\n",
        "        with torch.no_grad():\r\n",
        "            while predicted_index not in break_tokens:\r\n",
        "                outputs = model(tokens_tensor)\r\n",
        "                predictions = outputs[0]\r\n",
        "                predicted_index = torch.argmax(predictions[0, -1, :]).item()\r\n",
        "                indexed_tokens += [predicted_index]\r\n",
        "                if len(indexed_tokens) > MAX_LEN:\r\n",
        "                    break\r\n",
        "\r\n",
        "                predicted_text = tokenizer.decode(indexed_tokens)\r\n",
        "                if '<|action|>' in predicted_text:\r\n",
        "                    generated_actions = predicted_text.split('<|action|>')[-1].split('<|endofaction|>')[0].split(',')\r\n",
        "                    new_actions = []\r\n",
        "                    for a in generated_actions:\r\n",
        "                        if a in ['', ' ']:\r\n",
        "                            continue\r\n",
        "                        new_actions.append(a.strip())\r\n",
        "                    len_actions = len(new_actions)\r\n",
        "                    if len(list(set(new_actions))) > len(new_actions) or (len_actions > 10 and not truncate_action):\r\n",
        "                        # ipdb.set_trace()\r\n",
        "                        actions = '<|action|> {} <|endofaction|>'.format(' , '.join(list(set(new_actions))))\r\n",
        "                        indexed_tokens = tokenizer.encode('{} {}'.format(predicted_text.split('<|action|>')[0], actions))\r\n",
        "                        # print('action truncated')\r\n",
        "                        truncate_action = True\r\n",
        "                tokens_tensor = torch.tensor([indexed_tokens]).to('cuda')\r\n",
        "\r\n",
        "        predicted_text = tokenizer.decode(indexed_tokens)\r\n",
        "\r\n",
        "        action_text = get_action_new(predicted_text)\r\n",
        "        response_text = get_response_new(predicted_text)\r\n",
        "        # print(predicted_text)\r\n",
        "\r\n",
        "        db_results = multiwoz_db.queryResultVenues_new(domain, beliefs[domain], real_belief=True)\r\n",
        "\r\n",
        "        if domain == 'train':\r\n",
        "            lex_response = lexicalize_train(response_text, db_results, beliefs, turn_domain=domain)\r\n",
        "        elif domain == 'hotel':\r\n",
        "            lex_response = lexicalize_hotel(response_text, db_results, beliefs, turn_domain=domain)\r\n",
        "        else:\r\n",
        "            ipdb.set_trace()\r\n",
        "            raise TypeError('unknown domain')\r\n",
        "\r\n",
        "        delex_system = '<|system|> {}'.format(response_text)\r\n",
        "        system = '<|system|> {}'.format(lex_response)\r\n",
        "        context = context + ' ' + system\r\n",
        "\r\n",
        "        print(Fore.CYAN + 'SimpleTOD: ', end=\"\")\r\n",
        "        for a in lex_response.split(' '):\r\n",
        "            print(a + ' ', end=\"\")\r\n",
        "            sys.stdout.flush()\r\n",
        "            time.sleep(delay)\r\n",
        "        print(Style.RESET_ALL)\r\n",
        "        print(Fore.YELLOW + 'belief: {}'.format(beliefs) + Style.RESET_ALL)\r\n",
        "\r\n",
        "        print(Style.RESET_ALL)\r\n",
        "\r\n",
        "        turn += 1\r\n",
        "        prev_beliefs = beliefs\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b]0;SimpleTOD\u0007\n",
            "Loading Model"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-176512f0019c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIGPTLMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1758\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m             )\n\u001b[0;32m-> 1760\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '-f'. Make sure that:\n\n- '-f' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '-f' is the correct path to a directory containing relevant tokenizer files\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJPR6w2e7tE7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}